<span style="color:rgb(0, 176, 80)">what is content discovery</span><span style="color:rgb(0, 176, 80)">?</span>
firstly, we should ask, in the context of web application security, what is content? content can be many thins, a file, video, picture, backup, a website feature.
when whe talk about content discovery, we're not talking about the obvious things we can see on a website; it's the things that aren't immediately presented to us and that weren't always intended for public access.
an example: pages or portals intended for staff usage, older versions of the website, backup files, configuration files, administration panels, etc.
there are three main wyays of discovering content on a website whice we'll cover, manually, automated and OSINT (Open-Source Intelligence).

<span style="color:rgb(0, 176, 80)">robots.txt</span>
the robots.txt file is a document that tells search engines which pages they are and aren't allowed to show on their search engine results or ban specific search engines from crawling the website altogether
its a great way to discover paths that the developers doesn't want us to have access, so we may access these paths to see what we can find there.

<span style="color:rgb(0, 176, 80)">favicon</span>
the favicos is a small icon displayerd in the browser's address bar or tab used for branding a website
sometimes when frameworks are used to build a website, a favicon is a part of the instalation that gets leftover, and if the website developer doesn't replace this with a custom one, this can give us a clue on what frameworks is in use. OWASP host a database of common framework icons that you can use to check against the targets favicon [https://wiki.owasp.org/index.php/OWASP_favicon_database](https://wiki.owasp.org/index.php/OWASP_favicon_database).

<span style="color:rgb(0, 176, 80)">practical exercise:</span>
on the attackbox, open firefox enter the url of the site, /sites/favicon here you'll see a basic website with a note saying "website coming soon..."

open the source page, search for images/favicon.ico

if you run the following command on the attackbox, it will download the favicon and get its md5 havssh, value which you can the lookup on the [https://wiki.owasp.org/index.php/OWASP_favicon_database](https://wiki.owasp.org/index.php/OWASP_favicon_database).

command:
```shell-session
curl https://static-labs.tryhackme.cloud/sites/favicon/images/favicon.ico | md5sum
```
you'll get the hash code and u can search the framework that they're using in the owasp favicon database


<span style="color:rgb(0, 176, 80)">s</span><span style="color:rgb(0, 176, 80)">i</span><span style="color:rgb(0, 176, 80)">t</span><span style="color:rgb(0, 176, 80)">ema</span><span style="color:rgb(0, 176, 80)">p</span><span style="color:rgb(0, 176, 80)">.xml</span>
unlike the robots.txt file, which restricts what search engine crawlers can look at, the sitemap.xml file gives a list of every file the website owner wishes to be listed on a search engine.
these can sometimes contain areas of the website that are a bit more difficult to navigate to or even list some old webpages that current site no longer uses but are still working behind the scenes.


HTTP headers
when we make requests to the web server, the server returns various HTTP headers.
these headers can sometimes contain useful information such as the webserver software and possibly the programming/scripting language in use.
in the below example, we can see the webserver is NGINX version 1.18.0 and runs PHP version 7.4.3.
Using this information, we could find vulnerable versions of sofware being used. Trye running the below curl command against the web server, where the -v switch enables verbose mode, which will output the headers (there might be something interesting!).

command
curl http://10.10.128.74 - v (the '-v' in the curl command stands for verbose mode. it provides detailed information about the request and response, including HTTP headers. This can help you discover hidden details like the X-FLAG header.)

![[Pasted image 20250326104204.png]]



framework stack
once you've established the framework of a website, either from the above favicon example or by looking for clues in the page source such as comments, copyright notices or credits, you can the locate the framework's website.

![[Pasted image 20250326142140.png]]
![[Pasted image 20250326142211.png]]
![[Pasted image 20250326142258.png]]
![[Pasted image 20250326142314.png]]

viewing the documentation page gives us the path of the framework's administration portal.



OSINT - google hacking / dorking
there are also external resources available that can help in discovering information about your target website; these resources are often referred to as OSINT or (Open-source-intelligence) as they're freely available tools that collect information:

google hacking / dorking
google hacking / dorking utilizes google's advanced search engine features, which allow you to pick out custom content. you can, for instance, pick out results from a certain domain name using the site: filter, for example (site:tryhackme.com) you can the match this up with certain search terms, say, for example, the word admin (site:tryhackme admin) this then would only return results from the tryhackme.com website whice contain the word admin its content, you can combine multiple filter as well. here is an example of more filters you can use:

filters: site, inurl, filetype, intitle
example: 
- site:tryhackme.com - returns results only from the specified website address
- inurl:admin - returns results that have the specified word in the URL
- filetype:pdf - returns results which are a particular file extension
- intitle:admin - returns results that contain the specified word in the title
More information about google hacking can be found here: [https://en.wikipedia.org/wiki/Google_hacking](https://en.wikipedia.org/wiki/Google_hacking)


OSINT - wappalyzer
wappalyzer (https://www.wappalyzer.com/) is an online tool and browser extension that helps identify what technologies a website uses, such as frameworks, content management systems (CMS), payment processors and much more, and it can even fid version numbers as well.


OSINT -  wayback machine
the wayback machine ([https://archive.org/web/](https://archive.org/web/)) is a historical archive of websites that dates back to the late 90s. you can search a domain name, and it will show you all the times the service scraped the page and saved the contents. this service can help uncover old pages that may still be active on the current website.


OSINT - github 
To understand GitHub, you first need to understand Git. Git is a **version control system** that tracks changes to files in a project. Working in a team is easier because you can see what each team member is editing and what changes they made to files. When users have finished making their changes, they commit them with a message and then push them back to a central location (repository) for the other users to then pull those changes to their local machines. GitHub is a hosted version of Git on the internet. Repositories can either be set to public or private and have various access controls. You can use GitHub's search feature to look for company names or website names to try and locate repositories belonging to your target. Once discovered, you may have access to source code, passwords or other content that you hadn't yet found.


OSINT - S3 buckets
s3 buckets are a storage service provided by amazon AWS, alowwing people to save files and even static website content in the cloud acessible over HTTP and HTTPS. the worner of the files can set access permissions to either make files public, private and ever writable. sometimes these access permissions are incorrectly set and inadvertently allow access to files that shouldn't be available to the public. the format of the s3 buckets is 
http(s)://**{name}.**[**s3.amazonaws.com**](http://s3.amazonaws.com/)
where {name} is decided by the owner, such as
tryhackme-assets.s3.amazonaws.com. 
s3 buckets can be discovered in many ways, such as finding the URLS in the website's page source, github repositories, or even automating the process.
one common automation method is by using the company name followed by common terms such as
{name}-assets, {name}-www, {name}-public, {name}-private, etc.



automated discovery
what is automated discovery?
automated discovery is the process of using tools to discover content rather than doing it manually.
this process usually contains hundreds, thousands or even millions of requests to a web serer. these requests check wheter a file or directory exists on a website, giving us access to resources we didn't previously know existed. this process is made possible by using a resource called worlists.

what are wordlists?
wordlists are just text files that contain a long list of commonly used words; they can cover many different use cases. for example, a password wordlist would include the most frequently used passwords, whereas we're looking for content in our case, so we'd require a list containing the most commonly used directory and file names. an excellent resource for wordlist that is preinstalled on the THM attackbox is:
[https://github.com/danielmiessler/SecLists](https://github.com/danielmiessler/SecLists) which Daniel Miessler curates.

automation tools
although there are many different content discovery tools available, all with their features and flaws, we're going tocover three which are preinstalled on our attack box, ffuf, dirb, and gobuster

**Using ffuf:**
```shell-session
user@machine$ ffuf -w /usr/share/wordlists/SecLists/Discovery/Web-Content/common.txt -u http://MACHINE_IP/FUZZ
```

**Using dirb:**
```shell-session
user@machine$ dirb http://MACHINE_IP/ /usr/share/wordlists/SecLists/Discovery/Web-Content/common.txt
```

**Using Gobuster:**
```shell-session
user@machine$ gobuster dir --url http://MACHINE_IP/ -w /usr/share/wordlists/SecLists/Discovery/Web-Content/common.txt
```
